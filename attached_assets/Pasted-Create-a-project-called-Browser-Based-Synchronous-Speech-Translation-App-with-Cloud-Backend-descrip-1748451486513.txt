Create a project called Browser-Based Synchronous Speech Translation App with Cloud Backend, description: Я хочу построить приложение для синхронного перевода речи из браузера. Приложение должно работать с внешним микрофоном VB-cable или Blackhole. Пользователь должен уметь выбрать направление перевода, язык, с которого переводят, и язык, на который переводят. Также должно быть возможно выбрать голос озвучивания перевода. Перевод и озвучка должны осуществляться синхронно. Должна иметься возможность экспорта транскрипта, исходного текста и перевода. В положении должно поддерживаться расширенное логирование, и должна быть возможность экспорта логов.


Приложение должно иметь клиентскую часть, которая работает в браузере. И серверную часть, которая работает в облаке.
Клиентская часть захватывает голос, транскрибирует его и передает на сервер. 
На сервере происходит перевод полученного текста при помощи OpenAI API и отправка обратно на  в браузер на стороне клиента 
После чего переведенный текст озвучивается браузером с использованием Web Audio API. 

UI приложения должен быть современным, интерактивным, mobile friendly и SEO optimized.
Добавь в UI интерфейс приложения переключение UI с русского языка на английский, язык по умолчанию - русский.Также добавь в UI интерфейс приложения переключатель со светлой темы на темную, по умолчанию тема определяется настройками браузера.
Обязательными для каждого пользователя должны быть следующие настройки в UI:
- Настройки перевода: Source Language, Target Language, Voice
- Настройки модели: openai модель и Openai API Key
По каждой из настроек дай возможность выбора:
- Список поддерживаемых Source Language, Target language и Voice определяются тем, что поддерживает Web Audio API браузера
- Список Openai моделей определяется доступными на данный момент моделями openai
Сохраняй в кэше браузера настройки перевода: Source Language, Target Language, Voice а также настройку Openai API key и модели Openai которая используется для перевода, refer to the following documentation:

# Unified Project Documentation

## Project Requirements Document

### 1. Project Overview
This application is a browser-based tool for synchronous speech translation. It captures audio from a virtual microphone (like VB-cable or Blackhole), transcribes it in real time, sends the text to a cloud server for translation via the OpenAI API, and then plays back the translated speech through the browser’s built-in audio engine. Users can choose their source language, target language, and voice, and hear translations almost immediately after they speak.

The main goal is to make cross-language conversations seamless without installing special software. Success will be measured by low latency (under 500 ms per segment), high reliability (99.9% uptime), ease of use on desktop and mobile browsers, and full compliance with privacy rules (GDPR/CCPA). We’re building this because existing solutions often need desktop apps or separate hardware, and we want a simple, free, public tool right in the browser.

### 2. In-Scope vs. Out-of-Scope

**In-Scope (Version 1):**
- Real-time speech capture and transcription in the browser
- Cloud-based translation via OpenAI API
- Synchronous speech synthesis using Web Audio API
- User controls for start/stop recording
- Settings panel for Source Language, Target Language, Voice, OpenAI model, and API Key
- Caching of user settings in browser localStorage
- Light/dark theme toggle and Russian/English UI switch
- Export of transcripts, original text, translations, and logs in .txt, .json, .csv formats
- Privacy notice and microphone-use consent modal
- Automatic retry logic (up to 3 times) and user-friendly error messages

**Out-of-Scope (Deferred for later):**
- User accounts and server-side storage of transcripts or logs
- Paid or subscription billing flows
- Multi-user access levels (admin vs. regular users)
- Offline mode or local-only translation
- Advanced analytics dashboards or usage reporting

### 3. User Flow
When users first arrive, they see a modal explaining the privacy policy and microphone-use notice. After consenting, they land on a clean dashboard with controls for selecting source and target languages, voice options, OpenAI model, and their API key. The interface defaults to Russian and light/dark mode based on the browser’s setting. Users can switch UI language to English or toggle theme at any time.

To translate speech, the user clicks “Start,” grants microphone permission if needed, and begins speaking. The app transcribes the speech, streams it to the cloud server, gets a translation back, and immediately plays it in the chosen voice. When done, the user clicks “Stop” and can export transcripts, translations, and detailed logs. Any errors trigger clear messages with an automatic retry menu.

### 4. Core Features
- **Microphone Integration & Control:** Visual Start/Stop button; support for VB-cable or Blackhole virtual mics.
- **Real-Time Transcription:** Browser-based speech-to-text streaming to the server.
- **Cloud Translation:** OpenAI API calls for translating each transcription chunk.
- **Speech Synthesis & Playback:** Immediate playback using Web Audio API voices.
- **Settings Management:** UI panel for language, voice, OpenAI model, and API key with localStorage caching.
- **Export Functionality:** Download transcripts, source text, translations, and logs in .txt, .json, .csv formats.
- **Theming & Localization:** Light/dark mode; UI language switch between Russian and English.
- **Error Handling & Retry:** Automatic retry up to three times; clear error banners; manual retry.
- **Logging & Privacy Compliance:** Detailed session logs; privacy modal; no server-side storage.

### 5. Tech Stack & Tools
- **Frontend:** React (with Vite), TypeScript, Tailwind CSS, Shadcn UI, Web Speech API (for transcription), Web Audio API (for playback), localStorage API (for caching).
- **Backend:** Node.js with Express, OpenAI Node.js SDK, hosted on Railway.app.
- **AI Models:** OpenAI GPT-3.5 or GPT-4 for translation, selectable by the user.
- **Deployment & Tools:** Git for version control, Railway CI/CD for auto-deployments, Vite for front-end build.

### 6. Non-Functional Requirements
- **Performance:** Translation roundtrip under 500 ms per segment; UI updates in under 100 ms.
- **Availability:** 99.9% uptime; automatic retries for transient errors.
- **Security & Privacy:** All data sent over HTTPS; no server-side storage of personal data; localStorage encrypted at rest by the browser.
- **Compliance:** GDPR/CCPA consent capture; privacy and microphone modal.

### 7. Constraints & Assumptions
- Relies on modern browsers supporting Web Speech API and Web Audio API.
- Users must supply a valid OpenAI API key; no built-in billing or key management.
- No user authentication or persistent server storage.
- Hosting on Railway.app with standard resource limits.
- Browser caching only; clearing data resets settings.

### 8. Known Issues & Potential Pitfalls
- **Browser API Inconsistency:** Different speech APIs across browsers. Mitigation: detect and warn, provide fallback instructions.
- **Virtual Mic Configuration:** Users may misconfigure VB-cable or Blackhole. Mitigation: include a setup guide.
- **OpenAI Rate Limits:** Heavy use may hit API limits. Mitigation: show rate-limit errors, advise slower speech or upgrade key.
- **Network Drops:** Translations fail if offline. Mitigation: retry logic and clear messages.
- **Latency Variability:** Backend speed may vary. Mitigation: caching, batching small segments.

## App Flow Document

### Onboarding and Sign-In/Sign-Up
When a new user lands on the page, a modal pops up explaining the privacy policy, data handling, and microphone-use notice. The user must click “I Agree” to proceed. There is no sign-up or login; the app works as a public tool. After consent, the modal closes and the user sees the main interface.

### Main Dashboard or Home Page
On the main screen, a header shows the app title, a language toggle (Russian/English), and a theme switch (light/dark). Below, dropdowns let the user pick source language, target language, and voice from the browser’s available options. Next to that, fields allow entry of the OpenAI API key and selection of the model. A large Start/Stop button sits prominently in the center. A transcript area below shows live text and translated text side by side.

### Detailed Feature Flows and Page Transitions
When the user selects settings, the choices are saved immediately to localStorage. Clicking “Start” prompts for microphone permission if needed. Once granted, the button changes to “Stop” and the transcription area begins filling with live text. Each chunk of transcribed text is sent to the server via HTTP. The backend translates the text and sends it back. The client then queues the translation for playback and highlights the corresponding translated text as it speaks.

If the user clicks “Stop,” recording ends, but translation may finish playing. The transcript area pauses but remains visible. The user can then click an “Export” button to download files. Each download option opens a simple file-save dialog for .txt, .json, or .csv formats without navigating away.

### Settings and Account Management
At any point, the user can open the Settings panel to change source or target language, voice, OpenAI model, or API key. Changes take effect immediately and are saved to localStorage. After adjusting settings, the user closes the panel and returns to the main flow without losing any data in progress.

### Error States and Alternate Paths
If transcription, translation, or playback fails, a banner appears at the top indicating the problem (“Translation service unavailable,” “Network error,” etc.). The app will automatically retry up to three times. If retries all fail, the banner changes to include a “Retry” button that restarts the failed operation. During network loss, the Start button is disabled until connectivity returns.

### Conclusion and Overall App Journey
From landing to export, the entire journey is a single-page experience. Users consent, pick settings, speak, see live transcription, hear translations, and download their files. The flow is designed for speed, clarity, and minimal clicks, enabling seamless real-time translation in the browser.

## Tech Stack Document

### Frontend Technologies
- **Vite + React + TypeScript:** Fast build times and type safety for scalable code.
- **Tailwind CSS & Shadcn UI:** Utility-first styling with prebuilt components for quick, consistent UI.
- **Web Speech API:** Browser’s native speech-to-text engine for real-time transcription.
- **Web Audio API:** Native audio synthesis for immediate playback of translated text.
- **localStorage API:** Simple key/value storage for persisting user settings between sessions.

### Backend Technologies
- **Node.js & Express:** Lightweight HTTP server for translation requests.
- **OpenAI Node.js SDK:** Easy integration with OpenAI for translation models.
- **Railway.app:** Managed cloud hosting with CI/CD for automatic deployments.

### Infrastructure and Deployment
- **Railway.app:** Auto-scaling, simple environment management, and deployment rollbacks.
- **GitHub & Railway CI/CD:** Push-to-deploy workflow for front-end and back-end.
- **Vite Build Output:** Static assets served via Railway’s CDN for global performance.

### Third-Party Integrations
- **OpenAI API:** Core translation service using GPT-3.5/GPT-4 models.
- **Browser APIs:** No additional SDKs needed for speech or audio; reduces overhead.

### Security and Performance Considerations
- **HTTPS Everywhere:** All client/server communication secured by TLS.
- **CORS & Rate-Limit Guards:** Prevent unauthorized API use and manage traffic.
- **Client-Side Only Storage:** No PII on server; localStorage clears on user’s browser.
- **Lazy Loading & Code Splitting:** Only load essential code on first paint; split speech and translation modules.

### Conclusion and Overall Tech Stack Summary
This stack combines modern front-end tooling (Vite, React, Tailwind) with a simple Node/Express backend on Railway. It leverages built-in browser APIs for speech and audio, keeping dependencies light and performance high. OpenAI powers the translation, while localStorage ensures a server-less user experience that respects privacy.

## Frontend Guideline Document

### Frontend Architecture
We use a component-based React architecture powered by Vite for lightning-fast dev cycles. Each UI element—buttons, dropdowns, panels—is a separate component in a `/components` folder. Page-level components live in `/pages` or top-level containers. This makes the code easy to maintain and extend as features grow.

### Design Principles
We follow mobile-first, responsive design so the app works on all screen sizes. Accessibility is key: all controls have proper labels, keyboard access, and contrast for readability. Usability means clear icons, minimal steps, and instant feedback for actions like recording or errors.

### Styling and Theming
Tailwind CSS provides utility classes, and we define CSS variables for colors to support light/dark themes. We use a neutral palette (grays, blues) and a sans-serif font like Roboto or Open Sans for clarity. The theme toggle switches a `data-theme` attribute, and Tailwind’s `dark:` variants handle styling automatically.

### Component Structure
Components are organized by function: core UI primitives in `/components/ui`, feature containers in `/components/feature`, and shared hooks in `/hooks`. This separation ensures reusability and clear boundaries between generic controls and app-specific screens.

### State Management
We keep component-level state with React’s `useState` and share global settings using React Context. A custom `SettingsContext` reads/writes localStorage so preferences persist. This avoids heavy libraries while keeping state predictable.

### Routing and Navigation
The app is single-page, but we use React Router if we expand to multiple views (e.g., `/privacy`, `/error`). For now, navigation is managed by showing and hiding panels. This keeps the UI fast and simple.

### Performance Optimization
We lazy-load speech and translation modules only when the user clicks “Start.” Code splitting ensures the initial bundle is small. We use React’s `memo` and `useCallback` to reduce unnecessary renders, and Tailwind’s purge tool removes unused CSS.

### Testing and Quality Assurance
Unit tests with Jest and React Testing Library cover core components and logic. We use Cypress (or Playwright) for end-to-end tests simulating recording, translation, playback, and export flows. Lighthouse audits ensure performance, accessibility, and SEO standards are met.

### Conclusion and Overall Frontend Summary
Our front end is a modern, component-driven React app that prioritizes speed, accessibility, and ease of maintenance. Tailwind CSS and Vite keep the bundle lean, while React Context and localStorage handle state simply. The setup supports growth and ensures users get a fast, reliable translation experience.

## Implementation Plan
1. **Setup Repository & Tools:** Initialize Git repo; scaffold Vite + React + TypeScript; install Tailwind CSS, Shadcn UI.
2. **Theme & Localization Toggles:** Build header controls for light/dark theme and RU/EN text; wire up i18n strings.
3. **Settings Panel:** Create settings component with dropdowns for languages, voices, OpenAI model, and API key; sync with localStorage.
4. **Audio Capture UI:** Implement Start/Stop button; request microphone permission and capture stream from virtual mic.
5. **Transcription Integration:** Hook into Web Speech API for real-time transcription; display live text.
6. **Backend Service:** Deploy Node.js + Express server on Railway; add `/translate` endpoint using OpenAI SDK.
7. **Client-Server Communication:** Use HTTP (or WebSocket) to send transcription chunks and receive translations.
8. **Speech Synthesis:** Feed translated text into Web Audio API; play back with highlighted text.
9. **Export Functionality:** Develop export module to download .txt, .json, .csv files for transcripts, translations, logs.
10. **Error Handling & Retries:** Add retry logic, error banners, and manual retry buttons on failures.
11. **Logging:** Capture session events (start/stop, API calls, errors) and include in downloadable logs.
12. **Privacy Modal:** Present privacy and microphone-use notice on first load; block UI until consent.
13. **Testing:** Write unit tests for components, e2e tests for full flow; run Lighthouse audits.
14. **Deployment:** Configure Railway CI/CD for backend and Vercel (or Railway static host) for front end; verify DNS and HTTPS.
15. **Review & Polish:** Final QA, documentation check, performance tuning, and prepare launch.

---
*All sections above form a unified reference for developers and AI agents to build, test, and extend the synchronous speech translation app.*